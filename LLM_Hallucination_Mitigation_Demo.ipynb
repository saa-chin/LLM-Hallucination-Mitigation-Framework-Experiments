{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layered Framework for LLM Hallucination Mitigation\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yourusername/multi-layered-llm-hallucination-mitigation/blob/main/LLM_Hallucination_Mitigation_Demo.ipynb)\n",
    "\n",
    "This notebook provides an interactive demonstration of our multi-layered hallucination mitigation framework. You can run this directly in Google Colab or locally with Jupyter.\n",
    "\n",
    "**Paper:** Hiriyanna, S., & Zhao, W. (2025). Multi-Layered Framework for LLM Hallucination Mitigation in High-Stakes Applications: A Tutorial. *Computers*, MDPI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's install the required dependencies and set up your API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q openai chromadb numpy tiktoken sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For API key input\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. API Key Configuration\n",
    "\n",
    "Enter your OpenAI API key below. It will be hidden for security."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Securely input your OpenAI API key\n",
    "OPENAI_API_KEY = getpass('Enter your OpenAI API key: ')\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "\n",
    "# Initialize OpenAI client\n",
    "import openai\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "print(\"‚úì API key configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Framework Implementation\n",
    "\n",
    "### 3.1 Domain Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Domain(Enum):\n",
    "    GENERAL = \"general\"\n",
    "    FINANCIAL = \"financial\"\n",
    "    COMPLIANCE = \"compliance\"\n",
    "\n",
    "@dataclass\n",
    "class DomainConfig:\n",
    "    domain: Domain\n",
    "    confidence_threshold: float\n",
    "    keywords: List[str]\n",
    "\n",
    "# Domain configurations with confidence thresholds\n",
    "DOMAIN_CONFIGS = {\n",
    "    Domain.FINANCIAL: DomainConfig(\n",
    "        domain=Domain.FINANCIAL,\n",
    "        confidence_threshold=0.80,\n",
    "        keywords=[\"fee\", \"investment\", \"fund\", \"rate\", \"return\", \"portfolio\", \"401k\", \"IRA\"]\n",
    "    ),\n",
    "    Domain.COMPLIANCE: DomainConfig(\n",
    "        domain=Domain.COMPLIANCE,\n",
    "        confidence_threshold=0.85,\n",
    "        keywords=[\"regulation\", \"compliance\", \"legal\", \"policy\", \"rule\", \"requirement\"]\n",
    "    ),\n",
    "    Domain.GENERAL: DomainConfig(\n",
    "        domain=Domain.GENERAL,\n",
    "        confidence_threshold=0.75,\n",
    "        keywords=[]\n",
    "    )\n",
    "}\n",
    "\n",
    "def classify_domain(query: str) -> Domain:\n",
    "    \"\"\"Classify query into domain based on keywords.\"\"\"\n",
    "    query_lower = query.lower()\n",
    "    \n",
    "    for domain, config in DOMAIN_CONFIGS.items():\n",
    "        if domain != Domain.GENERAL:\n",
    "            if any(keyword in query_lower for keyword in config.keywords):\n",
    "                return domain\n",
    "    \n",
    "    return Domain.GENERAL\n",
    "\n",
    "print(\"Domain classification module loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Prompt Engineering Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptEngineer:\n",
    "    \"\"\"Implements prompt engineering techniques.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_few_shot_prompt(query: str, examples: List[Dict[str, str]]) -> str:\n",
    "        \"\"\"Create few-shot prompt with examples.\"\"\"\n",
    "        prompt = \"You are a helpful assistant. Here are some examples:\\n\\n\"\n",
    "        \n",
    "        for example in examples:\n",
    "            prompt += f\"Q: {example['question']}\\n\"\n",
    "            prompt += f\"A: {example['answer']}\\n\\n\"\n",
    "        \n",
    "        prompt += f\"Q: {query}\\nA:\"\n",
    "        return prompt\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_role_prompt(query: str, role: str) -> str:\n",
    "        \"\"\"Create role-playing prompt.\"\"\"\n",
    "        return f\"\"\"You are {role}. \n",
    "Your responses must be accurate, grounded in facts, and acknowledge uncertainty when appropriate.\n",
    "If you're not certain about something, say so clearly.\n",
    "\n",
    "Question: {query}\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_cot_prompt(query: str) -> str:\n",
    "        \"\"\"Create chain-of-thought prompt.\"\"\"\n",
    "        return f\"\"\"Let's think about this step-by-step.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Please reason through this systematically:\n",
    "1. First, identify what information is being requested\n",
    "2. Consider what facts are relevant\n",
    "3. Think about any limitations or uncertainties\n",
    "4. Provide your answer based on this reasoning\n",
    "\n",
    "Reasoning and Answer:\"\"\"\n",
    "\n",
    "prompt_engineer = PromptEngineer()\n",
    "print(\"Prompt engineering layer loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 RAG Layer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample knowledge base for demonstration\n",
    "SAMPLE_KNOWLEDGE_BASE = [\n",
    "    {\n",
    "        \"id\": \"doc_1\",\n",
    "        \"content\": \"The Quantum Investment Fund has an annual management fee of 0.75% and a front-load fee of 2%. The minimum investment is $10,000.\",\n",
    "        \"metadata\": {\"source\": \"Fund Prospectus\", \"type\": \"product_info\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_2\",\n",
    "        \"content\": \"Professional Trader Accounts require: minimum annual income of $150,000 OR net worth of $1,000,000, high risk tolerance, and at least 5 years of trading experience.\",\n",
    "        \"metadata\": {\"source\": \"Account Requirements\", \"type\": \"compliance\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_3\",\n",
    "        \"content\": \"Our Global Stability Fund focuses on low-risk investments with an annual management fee of 0.50% and no load fees. Suitable for conservative investors.\",\n",
    "        \"metadata\": {\"source\": \"Fund Information\", \"type\": \"product_info\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_4\",\n",
    "        \"content\": \"All investment recommendations must include risk disclosures and past performance warnings as per SEC regulations.\",\n",
    "        \"metadata\": {\"source\": \"Compliance Manual\", \"type\": \"compliance\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "class SimpleRAG:\n",
    "    \"\"\"Simplified RAG implementation for demonstration.\"\"\"\n",
    "    \n",
    "    def __init__(self, documents: List[Dict[str, Any]]):\n",
    "        self.documents = documents\n",
    "        self.client = openai.OpenAI()\n",
    "    \n",
    "    def get_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"Get embedding for text using OpenAI.\"\"\"\n",
    "        response = self.client.embeddings.create(\n",
    "            model=\"text-embedding-ada-002\",\n",
    "            input=text\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    \n",
    "    def cosine_similarity(self, a: List[float], b: List[float]) -> float:\n",
    "        \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "        a_np = np.array(a)\n",
    "        b_np = np.array(b)\n",
    "        return np.dot(a_np, b_np) / (np.linalg.norm(a_np) * np.linalg.norm(b_np))\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 3, threshold: float = 0.75) -> Tuple[List[Dict], float]:\n",
    "        \"\"\"Retrieve relevant documents and calculate confidence.\"\"\"\n",
    "        query_embedding = self.get_embedding(query)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = []\n",
    "        for doc in self.documents:\n",
    "            doc_embedding = self.get_embedding(doc['content'])\n",
    "            similarity = self.cosine_similarity(query_embedding, doc_embedding)\n",
    "            similarities.append((similarity, doc))\n",
    "        \n",
    "        # Sort by similarity\n",
    "        similarities.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        # Filter by threshold and get top-k\n",
    "        relevant_docs = []\n",
    "        scores = []\n",
    "        for similarity, doc in similarities[:top_k]:\n",
    "            if similarity >= threshold:\n",
    "                relevant_docs.append(doc)\n",
    "                scores.append(similarity)\n",
    "        \n",
    "        # Calculate confidence (weighted average of similarities)\n",
    "        if scores:\n",
    "            weights = [1.0, 0.5, 0.33][:len(scores)]\n",
    "            confidence = sum(s * w for s, w in zip(scores, weights)) / sum(weights)\n",
    "        else:\n",
    "            confidence = 0.0\n",
    "        \n",
    "        return relevant_docs, confidence\n",
    "\n",
    "# Initialize RAG with sample knowledge base\n",
    "rag_system = SimpleRAG(SAMPLE_KNOWLEDGE_BASE)\n",
    "print(\"RAG layer initialized with sample knowledge base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Multi-Layered Framework Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HallucinationMitigationFramework:\n",
    "    \"\"\"Main framework combining all layers.\"\"\"\n",
    "    \n",
    "    def __init__(self, rag_system: SimpleRAG):\n",
    "        self.rag = rag_system\n",
    "        self.client = openai.OpenAI()\n",
    "        self.prompt_engineer = PromptEngineer()\n",
    "    \n",
    "    def process_query(self, query: str, verbose: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"Process query through the multi-layered framework.\"\"\"\n",
    "        \n",
    "        # Step 1: Domain Classification\n",
    "        domain = classify_domain(query)\n",
    "        threshold = DOMAIN_CONFIGS[domain].confidence_threshold\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"üìä Domain: {domain.value} (threshold: {threshold})\")\n",
    "        \n",
    "        # Step 2: RAG Retrieval\n",
    "        relevant_docs, confidence = self.rag.retrieve(query, threshold=0.7)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"üîç Retrieved {len(relevant_docs)} relevant documents\")\n",
    "            print(f\"üìà Confidence: {confidence:.3f}\")\n",
    "        \n",
    "        # Step 3: Decision based on confidence\n",
    "        if confidence < threshold:\n",
    "            if verbose:\n",
    "                print(f\"‚ö†Ô∏è Confidence below threshold - escalating to human\")\n",
    "            return {\n",
    "                \"status\": \"escalated\",\n",
    "                \"response\": \"I don't have sufficient information to answer this question accurately. Please contact a human representative for assistance.\",\n",
    "                \"confidence\": confidence,\n",
    "                \"domain\": domain.value\n",
    "            }\n",
    "        \n",
    "        # Step 4: Generate response with context\n",
    "        context = \"\\n\\n\".join([doc['content'] for doc in relevant_docs])\n",
    "        \n",
    "        # Apply prompt engineering\n",
    "        if domain == Domain.FINANCIAL:\n",
    "            role = \"a financial advisor assistant\"\n",
    "            prompt = self.prompt_engineer.create_role_prompt(query, role)\n",
    "        elif domain == Domain.COMPLIANCE:\n",
    "            prompt = self.prompt_engineer.create_cot_prompt(query)\n",
    "        else:\n",
    "            prompt = query\n",
    "        \n",
    "        # Add context to prompt\n",
    "        full_prompt = f\"\"\"Based on the following verified information:\n",
    "\n",
    "{context}\n",
    "\n",
    "{prompt}\n",
    "\n",
    "Important: Only use information from the provided context. If the answer is not in the context, say so.\"\"\"\n",
    "        \n",
    "        # Generate response\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-4-turbo-preview\",\n",
    "            messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
    "            temperature=0.0,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"‚úÖ Response generated successfully\")\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"response\": response.choices[0].message.content,\n",
    "            \"confidence\": confidence,\n",
    "            \"domain\": domain.value,\n",
    "            \"sources\": [doc['metadata']['source'] for doc in relevant_docs]\n",
    "        }\n",
    "\n",
    "# Initialize the framework\n",
    "framework = HallucinationMitigationFramework(rag_system)\n",
    "print(\"‚úÖ Multi-layered framework initialized and ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interactive Demo\n",
    "\n",
    "Now let's test the framework with some example queries. You can modify these or add your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries demonstrating different scenarios\n",
    "TEST_QUERIES = [\n",
    "    \"What are the fees for the Quantum Investment Fund?\",\n",
    "    \"What is the minimum investment for the Global Stability Fund?\",\n",
    "    \"Am I eligible for a Professional Trader Account with $100,000 income?\",\n",
    "    \"Tell me about the XYZ fund\",  # Non-existent fund - should escalate\n",
    "    \"What is the weather today?\"  # Off-topic - should handle appropriately\n",
    "]\n",
    "\n",
    "print(\"Select a query to test or enter your own:\\n\")\n",
    "for i, query in enumerate(TEST_QUERIES, 1):\n",
    "    print(f\"{i}. {query}\")\n",
    "print(\"\\n0. Enter custom query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive query selection\n",
    "choice = input(\"\\nEnter your choice (0-5): \")\n",
    "\n",
    "if choice == \"0\":\n",
    "    query = input(\"Enter your custom query: \")\n",
    "else:\n",
    "    query = TEST_QUERIES[int(choice) - 1]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Process the query\n",
    "result = framework.process_query(query, verbose=True)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FRAMEWORK RESPONSE:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nStatus: {result['status']}\")\n",
    "print(f\"Domain: {result['domain']}\")\n",
    "print(f\"Confidence: {result['confidence']:.3f}\")\n",
    "if 'sources' in result:\n",
    "    print(f\"Sources: {', '.join(result['sources'])}\")\n",
    "print(f\"\\nResponse:\\n{result['response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batch Evaluation\n",
    "\n",
    "Let's run all test queries to see how the framework performs across different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_framework(queries: List[str]):\n",
    "    \"\"\"Evaluate framework on multiple queries.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"\\nProcessing: {query[:50]}...\")\n",
    "        result = framework.process_query(query, verbose=False)\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"status\": result['status'],\n",
    "            \"confidence\": result['confidence'],\n",
    "            \"domain\": result['domain']\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "print(\"Running batch evaluation...\\n\")\n",
    "evaluation_results = evaluate_framework(TEST_QUERIES)\n",
    "\n",
    "# Display results in a table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"{'Query':<40} {'Status':<12} {'Domain':<12} {'Confidence':<10}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for result in evaluation_results:\n",
    "    query_short = result['query'][:37] + \"...\" if len(result['query']) > 40 else result['query']\n",
    "    print(f\"{query_short:<40} {result['status']:<12} {result['domain']:<12} {result['confidence']:.3f}\")\n",
    "\n",
    "# Calculate statistics\n",
    "successful = sum(1 for r in evaluation_results if r['status'] == 'success')\n",
    "escalated = sum(1 for r in evaluation_results if r['status'] == 'escalated')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Summary: {successful}/{len(evaluation_results)} successful, {escalated}/{len(evaluation_results)} escalated\")\n",
    "print(f\"Average confidence: {np.mean([r['confidence'] for r in evaluation_results]):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison with Baseline GPT-4\n",
    "\n",
    "Let's compare our framework's response with a baseline GPT-4 response (without RAG or confidence thresholds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline_response(query: str) -> str:\n",
    "    \"\"\"Get baseline GPT-4 response without framework.\"\"\"\n",
    "    client = openai.OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo-preview\",\n",
    "        messages=[{\"role\": \"user\", \"content\": query}],\n",
    "        temperature=0.0,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Compare on a specific query\n",
    "test_query = \"What are the fees for the Quantum Investment Fund?\"\n",
    "\n",
    "print(f\"Query: {test_query}\\n\")\n",
    "print(\"=\"*60)\n",
    "print(\"BASELINE GPT-4 RESPONSE:\")\n",
    "print(\"=\"*60)\n",
    "baseline_response = get_baseline_response(test_query)\n",
    "print(baseline_response)\n",
    "print(f\"\\nLength: {len(baseline_response)} characters\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FRAMEWORK RESPONSE:\")\n",
    "print(\"=\"*60)\n",
    "framework_result = framework.process_query(test_query, verbose=False)\n",
    "print(framework_result['response'])\n",
    "print(f\"\\nLength: {len(framework_result['response'])} characters\")\n",
    "print(f\"Confidence: {framework_result['confidence']:.3f}\")\n",
    "print(f\"Sources: {', '.join(framework_result.get('sources', []))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Custom Knowledge Base\n",
    "\n",
    "You can add your own documents to the knowledge base. Try adding domain-specific information below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom documents to the knowledge base\n",
    "def add_custom_document():\n",
    "    print(\"Add a custom document to the knowledge base\\n\")\n",
    "    \n",
    "    content = input(\"Enter document content: \")\n",
    "    source = input(\"Enter source name: \")\n",
    "    doc_type = input(\"Enter document type (product_info/compliance/general): \")\n",
    "    \n",
    "    new_doc = {\n",
    "        \"id\": f\"custom_{len(rag_system.documents) + 1}\",\n",
    "        \"content\": content,\n",
    "        \"metadata\": {\"source\": source, \"type\": doc_type}\n",
    "    }\n",
    "    \n",
    "    rag_system.documents.append(new_doc)\n",
    "    print(f\"\\n‚úÖ Document added successfully! Knowledge base now has {len(rag_system.documents)} documents.\")\n",
    "    \n",
    "    # Test with a query\n",
    "    test_query = input(\"\\nEnter a query to test the new document (or press Enter to skip): \")\n",
    "    if test_query:\n",
    "        result = framework.process_query(test_query, verbose=True)\n",
    "        print(f\"\\nResponse: {result['response']}\")\n",
    "\n",
    "# Uncomment to add a custom document\n",
    "# add_custom_document()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Metrics\n",
    "\n",
    "Let's calculate some performance metrics similar to those reported in our paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(queries: List[str]):\n",
    "    \"\"\"Calculate performance metrics for the framework.\"\"\"\n",
    "    \n",
    "    metrics = {\n",
    "        \"total_queries\": len(queries),\n",
    "        \"successful\": 0,\n",
    "        \"escalated\": 0,\n",
    "        \"avg_confidence\": [],\n",
    "        \"response_lengths\": [],\n",
    "        \"domains\": {\"general\": 0, \"financial\": 0, \"compliance\": 0}\n",
    "    }\n",
    "    \n",
    "    for query in queries:\n",
    "        result = framework.process_query(query, verbose=False)\n",
    "        \n",
    "        if result['status'] == 'success':\n",
    "            metrics['successful'] += 1\n",
    "        else:\n",
    "            metrics['escalated'] += 1\n",
    "        \n",
    "        metrics['avg_confidence'].append(result['confidence'])\n",
    "        metrics['response_lengths'].append(len(result['response']))\n",
    "        metrics['domains'][result['domain']] += 1\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    metrics['success_rate'] = metrics['successful'] / metrics['total_queries'] * 100\n",
    "    metrics['escalation_rate'] = metrics['escalated'] / metrics['total_queries'] * 100\n",
    "    metrics['avg_confidence'] = np.mean(metrics['avg_confidence'])\n",
    "    metrics['avg_response_length'] = np.mean(metrics['response_lengths'])\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Calculate metrics\n",
    "print(\"Calculating performance metrics...\\n\")\n",
    "metrics = calculate_metrics(TEST_QUERIES)\n",
    "\n",
    "# Display metrics\n",
    "print(\"Framework Performance Metrics\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Total Queries: {metrics['total_queries']}\")\n",
    "print(f\"Success Rate: {metrics['success_rate']:.1f}%\")\n",
    "print(f\"Escalation Rate: {metrics['escalation_rate']:.1f}%\")\n",
    "print(f\"Average Confidence: {metrics['avg_confidence']:.3f}\")\n",
    "print(f\"Average Response Length: {metrics['avg_response_length']:.0f} characters\")\n",
    "print(\"\\nDomain Distribution:\")\n",
    "for domain, count in metrics['domains'].items():\n",
    "    print(f\"  {domain}: {count} queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualization\n",
    "\n",
    "Let's create a simple visualization of the framework's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create performance visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Success vs Escalation\n",
    "axes[0].pie([metrics['successful'], metrics['escalated']], \n",
    "            labels=['Successful', 'Escalated'],\n",
    "            autopct='%1.1f%%',\n",
    "            colors=['#2ecc71', '#e74c3c'])\n",
    "axes[0].set_title('Response Status Distribution')\n",
    "\n",
    "# Plot 2: Domain Distribution\n",
    "domains = list(metrics['domains'].keys())\n",
    "counts = list(metrics['domains'].values())\n",
    "axes[1].bar(domains, counts, color=['#3498db', '#f39c12', '#9b59b6'])\n",
    "axes[1].set_title('Query Domain Distribution')\n",
    "axes[1].set_xlabel('Domain')\n",
    "axes[1].set_ylabel('Number of Queries')\n",
    "\n",
    "# Plot 3: Confidence Distribution\n",
    "confidence_data = [framework.process_query(q, verbose=False)['confidence'] for q in TEST_QUERIES]\n",
    "axes[2].hist(confidence_data, bins=10, color='#1abc9c', edgecolor='black')\n",
    "axes[2].axvline(x=0.75, color='r', linestyle='--', label='General Threshold')\n",
    "axes[2].axvline(x=0.80, color='orange', linestyle='--', label='Financial Threshold')\n",
    "axes[2].axvline(x=0.85, color='purple', linestyle='--', label='Compliance Threshold')\n",
    "axes[2].set_title('Confidence Score Distribution')\n",
    "axes[2].set_xlabel('Confidence Score')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization complete! The charts show:\")\n",
    "print(\"1. Distribution of successful vs escalated responses\")\n",
    "print(\"2. Distribution of queries across different domains\")\n",
    "print(\"3. Confidence score distribution with domain-specific thresholds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps\n",
    "\n",
    "Congratulations! You've successfully run the Multi-Layered Hallucination Mitigation Framework. Here's what we've demonstrated:\n",
    "\n",
    "### Key Features:\n",
    "- **Domain Classification**: Automatically categorizes queries and applies appropriate thresholds\n",
    "- **RAG Integration**: Grounds responses in verified knowledge base documents\n",
    "- **Confidence-Based Escalation**: Knows when to defer to human experts\n",
    "- **Prompt Engineering**: Uses role-playing and chain-of-thought for better responses\n",
    "\n",
    "### Results:\n",
    "- Significantly reduced hallucinations compared to baseline GPT-4\n",
    "- More concise and accurate responses\n",
    "- Appropriate escalation for uncertain queries\n",
    "\n",
    "### Next Steps:\n",
    "1. **Expand the Knowledge Base**: Add more domain-specific documents\n",
    "2. **Fine-tune Thresholds**: Adjust confidence thresholds based on your use case\n",
    "3. **Production Deployment**: Integrate with your existing systems\n",
    "4. **Monitor Performance**: Track metrics over time\n",
    "\n",
    "### Citation:\n",
    "If you use this framework in your research or production systems, please cite:\n",
    "\n",
    "```bibtex\n",
    "@article{hiriyanna2025multilayered,\n",
    "    title={Multi-Layered Framework for LLM Hallucination Mitigation in High-Stakes Applications: A Tutorial},\n",
    "    author={Hiriyanna, Sachin and Zhao, Wenbing},\n",
    "    journal={Computers},\n",
    "    publisher={MDPI},\n",
    "    year={2025}\n",
    "}\n",
    "```\n",
    "\n",
    "For questions or support, contact:\n",
    "- Sachin Hiriyanna: sachinh@ieee.org\n",
    "- Wenbing Zhao: wenbing@ieee.org"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}